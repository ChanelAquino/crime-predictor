---
title: "Predicting heart disease with classification trees"
author: "Dario Molina"
date: "April 11, 2017"
output: html_document
---

<!-- change echo=FALSE to echo=TRUE to show code -->
```{r global_options, include=FALSE}
knitr::opts_chunk$set(prompt=TRUE, comment="", echo=TRUE)
```


###Introduction
With this report, heart disease data will be analyzed along with tree classification to predict whether a person has heart disease or not.

```{r collapse=TRUE, warning=FALSE}
library(rpart)
library(rpart.plot)
library(maptree)
library(caret)
# the following utility files can be found attached to the assignment
source("https://raw.githubusercontent.com/grbruns/cst383/master/lin-regr-util.R")
source("https://raw.githubusercontent.com/grbruns/cst383/master/class-util.R")
```

### Reading and preprocessing the data

Obtaining data from https://raw.githubusercontent.com/grbruns/cst383/master/heart.dat. The column names of the data frame have been set and converted the range of heart output to 0 and 1.  0 representing the person does not have heart disease and vice versa.

```{r}
 heart = read.table("https://raw.githubusercontent.com/grbruns/cst383/master/heart.dat", quote = "/")
 names(heart) <- c("AGE", "SEX", "CHESTPAIN", "RESTBP", "CHOL",
                   "SUGAR", "ECG", "MAXHR", "ANGINA", "DEP", "EXERCISE", "FLUOR",
                   "THAL", "OUTPUT")
 names(heart) = tolower(names(heart))
 
 # convert output to 0-1 range
 bad.ouput = heart$output
 heart$output = heart$output - 1  
 
 # categorical variables to factors
 heart$sex = factor(heart$sex)
 heart$sugar = factor(heart$sugar)
 heart$angina = factor(heart$angina)
 heart$chestpain = factor(heart$chestpain)
 heart$ecg = factor(heart$ecg)
 heart$thal = factor(heart$thal)
 heart$exercise = factor(heart$exercise)

# convert output to factor
heart$output = factor(heart$output)
```

### Data exploration

Most of the features are numerical, with the exception of output being categorical. There are only 270 rows in the data set, with 14 variables.


The mean age of the human subjects in the data is about 55 years old.
```{r}
hist(heart$age, main="age of test subjects", xlab="age", col="red4")
```

From a grid of scatterplots of the numeric features, it appears that no pairs of features are strongly correlated.
```{r}
plot(heart[,c("age","chestpain","restbp","maxhr","chol")])
```

Higher resting blood pressure and lower maximum heart rate appear to be associated with heart disease.
```{r}
 plot(heart$maxhr[heart$output == 1],heart$restbp[heart$output == 1], col="green", main = "heart disease by resting blood pressure and max heart rate", xlab = "maxhr",ylab="restbp")
 lines(heart$maxhr[heart$output == 0],heart$restbp[heart$output == 0], col="red", type ="p" )
  legend("topleft", legend=c("heart disease", "no heart disease"),col=c("red", "green"), lty=1:1, cex=0.8)
  
```


### Building a classification tree

From the scatterplot above, I'll utilize some of the features for our model.
```{r}
# training and test sets
set.seed(132)
split = split_data(heart)
tr_dat = split[[1]]
te_dat = split[[2]]


fit = rpart(output ~ chestpain + restbp + maxhr + chol,data=tr_dat, method="class")

prp(fit, extra=106, varlen=5,
 main="classification tree for heart disease",
 box.col=c("palegreen", "pink")[fit$frame$yval])

```


Based on the summary from fit, we can see the most important variable is chestpain, followed by maxhr. We can also see that for node 1, there were 201 observations and going to node 2 the number observations had a big drop.  For the following nodes the observations had small differences. It also seems like "chol" it isn't as relevant as the other features.


```{r}
summary(fit)
```
### Classifying test data

We run our model on the test data, and classify examples in the test data using a threshold of 0.5.

The summary output from our fit only gives a rough idea of the usefulness of our model. For a better idea we should use the model as a classifier and see how well it predicts heart disease on the test data.

```{r}

predicted = predict(fit, te_dat, type="prob")
predictors = as.data.frame(predicted)
names(predictors) = c("zero","one")
y = predictors$one
predicts = as.numeric(y > 0.5)
actuals = te_dat$output
confusion_matrix=table(actuals, predicts)
confusion_matrix
```
The accuracy of our classifier is not that bad, but is not that good either.


```{r}
 succ_rate = mean(predicts == actuals)
 round(succ_rate, 3)
```

### Assessing the model
Looking at the output of the model on test cases where heart disease is present, and on test cases where heart disease is not present, we can see that the model is not working especially well. We can also see that a classification threshold of about 0.5 is probably about right.
```{r}
par(mfrow=c(1,2))
 hist(predicted[actuals == 0], main="Output when no heart disease", breaks=10, xlim=c(0,1), ylim=c(0,15),col="red4", xlab="model predictions")

 hist(predicted[actuals == 1], main="Output when heart disease", breaks=10, xlim=c(0,1), ylim=c(0,15), col="red4", xlab="model predictions")
 
 
 par(mfrow=c(1,1))
 plot(density(predicted[actuals == 0]), main="double density plot", col="green",xlab="logistic regression output")
 lines(density(predicted[actuals == 1]),col="blue")
 abline(a=NULL,b=0,h=NULL,v=0.5,lty=4)
 legend("topright", legend=c("heart disease", "no heart disease"),col=c("blue", "green"), lty=1:1, cex=0.8)
```


```{r}

 prec_recall_summary = function(predicts, actuals) {
   thresh = seq(0, 1, length.out=69)
   prec_rec = data.frame()
   actuals = factor(as.numeric(actuals))
   for (th in thresh) {
     predicts = factor(as.numeric(y >= th), levels=c("0","1"))
     prec_rec = rbind(prec_rec, as.vector(table(predicts, actuals)))
   }
   names(prec_rec) = c("TN", "FP", "FN", "TP")
   prec_rec$threshold = thresh
   prec_rec$precision = prec_rec$TP/(prec_rec$TP + prec_rec$FP)
   prec_rec$recall    = prec_rec$TP/(prec_rec$TP + prec_rec$FN)
   prec_rec$false_pos = prec_rec$FP/(prec_rec$FP + prec_rec$TN)
   return(prec_rec)
 }

prec_rec1 = prec_recall_summary(predicts, actuals)
```

We can see that the precision of our model is not the best as we had seen from the accuracy, as well as the recall.
```{r}
plot(prec_rec1$threshold,prec_rec1$precision,xlab="Threshold",ylab="Precision", type="l",col="firebrick")
grid(5, 5, lwd = 2)

plot(prec_rec1$threshold,prec_rec1$recall,xlab="Threshold",ylab="Recall", type="l",col="firebrick")
grid(5, 5, lwd = 2)
```

The ROC plot suggests that the classifier is not working as well as expected.

The receiver operating characteristic (ROC) plot gives an overall idea of how well the classifier is working.
```{r}
plot(prec_rec1$false_pos,prec_rec1$TP, type = "l",col="firebrick",main="ROC",xlab="false positive rate",ylab="true positive rate")
```


###Learning Curve
```{r}
te_errs = c()
tr_errs = c()
te_actual = te_dat$output
tr_sizes = seq(100, nrow(tr_dat), length.out=20)




for (tr_size in tr_sizes) {
 tr_dat1 = tr_dat[1:tr_size,]
 tr_actual = tr_dat1$output
 fit = rpart(output ~ chestpain + restbp + maxhr + chol , method = "class" , data = tr_dat)


 # error on training set
 tr_predicted = predict(fit, tr_dat1, type="class")
 err = mean(tr_actual != tr_predicted)
 tr_errs = c(tr_errs, err)



 # error on test set
 te_predicted = predict(fit, te_dat, type="class")
 err = mean(te_actual != te_predicted)
 te_errs = c(te_errs, err)
}

plot(tr_errs,type = "b",col = "red", xlab = "Training set size", ylab = "Error (RMSE)",
     main = "Learning curve", ylim=c(0,0.5))
 lines(te_errs, type = "b", col = "blue")
 legend('topright', c("Train error", "Test error"), lty = c(1,1), lwd = c(2.5, 2.5),
        col = c("red", "blue"))
```

### Model 2

This model uses all input features
```{r}
fit2 = rpart(output ~ chestpain + thal + fluor + maxhr + angina + dep,data=tr_dat, method="class")

prp(fit2, extra=106, varlen=5,
 main="classification tree for public/private univ.",
 box.col=c("palegreen", "pink")[fit$frame$yval])
```


```{r}

predicted = predict(fit2, te_dat, type="prob")
predictors = as.data.frame(predicted)
names(predictors) = c("zero","one")
y = predictors$one
predicts = as.numeric(y > 0.5)
actuals = te_dat$output
confusion_matrix = table(actuals, predicts)
confusion_matrix
```
Confusion matrix for the model, given a threshold of 0.5.
```{r}
confusion_matrix
```

As we can see, the accuracy of the model is improved.
```{r}
 succ_rate = mean(predicts == actuals)
 round(succ_rate, 3)
```

Features ‘chestpain’ and ‘thal’ seem to be the most relevant predictors. 
```{r}
par(mfrow=c(1,2))
 hist(predicted[actuals == 0], main="Output when no heart disease", breaks=10, xlim=c(0,1), ylim=c(0,15),col="red4", xlab="model predictions")

 hist(predicted[actuals == 1], main="Output when heart disease", breaks=10, xlim=c(0,1), ylim=c(0,15), col="red4", xlab="model predictions")
```

Double density plot for the this model:
```{r}
 par(mfrow=c(1,1))
 plot(density(predicted[actuals == 0]), main="double density plot", col="green",xlab="logistic regression output")
 lines(density(predicted[actuals == 1]),col="blue")
 abline(a=NULL,b=0,h=NULL,v=0.5,lty=4)
 legend("topright", legend=c("heart disease", "no heart disease"),col=c("blue", "green"), lty=1:1, cex=0.8)
 
prec=prec_rec1
prec_rec1 = prec_recall_summary(predicts, actuals)
```


The ROC plot shows improved classifier performance, compared to our previous model.
```{r}
plot(prec_rec1$false_pos,prec_rec1$TP/25, type = "l",col="firebrick",main="ROC", xlab="false positive rate",ylab="true positive rate")
lines(prec$false_pos,prec$TP/25, type = "l",col="blue")
legend("bottomright", legend=c("model1", "model2"),col=c("blue", "red"), lty=1:1, cex=0.8)
```


###Learning Curve
```{r}
te_errs = c()
tr_errs = c()
te_actual = te_dat$output
tr_sizes = seq(100, nrow(tr_dat), length.out=20)




for (tr_size in tr_sizes) {
 tr_dat1 = tr_dat[1:tr_size,]
 tr_actual = tr_dat1$output
 fit = rpart(output ~ chestpain + thal + fluor + maxhr + angina + dep,data=tr_dat, method="class")


 # error on training set
 tr_predicted = predict(fit, tr_dat1, type="class")
 err = mean(tr_actual != tr_predicted)
 tr_errs = c(tr_errs, err)



 # error on test set
 te_predicted = predict(fit, te_dat, type="class")
 err = mean(te_actual != te_predicted)
 te_errs = c(te_errs, err)
}

plot(tr_errs,type = "b",col = "red", xlab = "Training set size", ylab = "Error (RMSE)",
     main = "Learning curve", ylim=c(0,0.5))
 lines(te_errs, type = "b", col = "blue")
 legend('topright', c("Train error", "Test error"), lty = c(1,1), lwd = c(2.5, 2.5),
        col = c("red", "blue"))
```


###Conclusion
We can see that our model is better fit when use only predictors that are relevant. Furthermore, it is important to notice the improvement from model2 compared to model1 based on the accuracy and roc derived from the data.
















